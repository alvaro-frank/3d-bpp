# 3D Bin Packing

A production-grade Deep Reinforcement Learning project that solves the 3D Bin Packing Problem. It implements **DQN** and **PPO** agents within a custom Gym environment, complete with heuristic baselines, 3D visualizations, and a production-ready API for recursive packing.

This project demonstrates a complete MLOps lifecycle: from training agents using PyTorch and tracking experiments with **MLflow**, to versioning artifacts with **DVC** and deploying a **FastAPI** inference service via **Docker**.

<p align="center">
    <img src="images/ppo_5000steps.gif" alt="PPO 5000 steps" width="600">
</p>

## ğŸ“‚ Project Structure
```
â”œâ”€â”€ .github/                 # CI/CD configuration
â”œâ”€â”€ .dvc/                    # DVC Configuration
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ agents/              # DQN & PPO Agent implementations
â”‚   â”œâ”€â”€ environment/         # Gym Env, Bin, and Box logic
â”‚   â”œâ”€â”€ evals/               # Evaluation scripts (Agent vs Heuristic)
â”‚   â”œâ”€â”€ heuristics/          # BLB Heuristic implementation
â”‚   â”œâ”€â”€ train/               # Training loops
â”‚   â”œâ”€â”€ utils/               # Visualization, Action Space, Box Generator
â”‚   â”œâ”€â”€ api.py               # FastAPI inference endpoints
â”‚   â””â”€â”€ main.py              # CLI Entrypoint
â”œâ”€â”€ tests/                   # Unit and Integration tests
â”œâ”€â”€ docker-compose.yml       # Docker services configuration
â”œâ”€â”€ Dockerfile               # Docker image definition
â”œâ”€â”€ Makefile                 # Command automation
â”œâ”€â”€ requirements.txt         # Python dependencies
â””â”€â”€ README.md                # Project documentation
```

## ğŸ› ï¸ Setup & Requirements

This project uses `make` for automation and `dvc` for data/artifact management.

---

## Quick Start
### ğŸš€ Full Pipeline

To train, evaluate, and visualize everything in one go:

```bash
make all
```

This will:
- Create the virtual environment and installs the required packages from `requirements.txt`.
- Train the RL agent (default: PPO).
- Evaluate it against fixed test sets and the heuristic baseline.
- Generate visualizations and logs.

You can also customize the run using arguments (see below).
### ğŸ Virtual Environment
This section explains how to create and activate the virtual environment and installs the required packages from `requirements.txt`, just use the command line:

```bash
make setup
```

### ğŸ§  Training
Train a RL agent (DQN or PPO) on the 3D Bin Packing environment:

```bash
make train
```

You can override defaults by passing variables on the command line:

| Arg        | Purpose                                   | Default | Examples |
|------------|-------------------------------------------|---------|----------|
| `AGENT`    | Which agent to train (`dqn` or `ppo`)     | `ppo`   | `AGENT=ppo` |
| `EPISODES` | Number of training episodes               | `200`   | `EPISODES=5000` |
| `BOXES`    | Number of boxes per episode               | `50`    | `BOXES=100` |
| `SEED`     | Random seed                               | `41`    | `SEED=123` |

Examples:
```bash
# Train PPO for longer
make train AGENT=ppo EPISODES=5000

# Train DQN with more boxes and a fixed seed
make train AGENT=dqn BOXES=80 SEED=7
```

Artifacts (per agent) are stored under `runs/<agent>/` (models, plots, GIFs).

---

### ğŸ“Š Evaluation
Evaluate a saved model against fixed test sets and the heuristic baseline:

```bash
make evaluate
```

You can target a specific checkpoint and adjust evaluation settings:

| Arg        | Purpose                                            | Default         | Examples |
|------------|----------------------------------------------------|-----------------|----------|
| `AGENT`    | Which agentâ€™s directory/model to use               | `ppo`           | `AGENT=ppo` |
| `MODEL`    | Path to a specific checkpoint to evaluate          | *(best/latest)* | `MODEL=runs/ppo/ppo_best.pt` |
| `TESTS`    | Number of evaluation episodes (test cases)         | `20`            | `TESTS=50` |
| `BOXES`    | Number of boxes per evaluation episode             | `50`            | `BOXES=100` |
| `SEED`     | Random seed for evaluation                         | `41`            | `SEED=123` |

Examples:
```bash
# Evaluate the latest PPO checkpoint on 50 tests
make evaluate AGENT=ppo TESTS=50

# Evaluate a specific model file with more boxes
make evaluate AGENT=dqn MODEL=runs/dqn/dqn_best.pt BOXES=80
```

By default, evaluation generates plots and 3D packing GIFs in `runs/<agent>/`.

---

### ğŸ“ˆ Experiment Tracking (MLflow)
This project uses **MLflow** to track training performance and version models.

**How to Launch the Dashboard**

To view training curves and logged artifacts, run the following command:
```bash
make mlflow PORT=5000
```
This will start the MLflow server at **http://127.0.0.1:5000** by default.

What is Logged?
Every time you run `make train`, a new experiment run is created logging:

**Metrics**:
- `volume_utilization`: % of bin volume filled.
- `boxes_placed`: Number of boxes successfully packed.
- `epsilon` (if DQN): Exploration rate decay.
- `avg_reward_100`: Moving average of the last 100 episodes.

**Artifacts**:
- Model Checkpoints: The final trained model is saved as an MLflow artifact.

---
